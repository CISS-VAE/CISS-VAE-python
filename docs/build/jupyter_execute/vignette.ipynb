{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use CISS-VAE\n",
    "\n",
    "# Overview\n",
    "\n",
    "The **Clustering-Informed Shared-Structure Variational Autoencoder (CISS-VAE)** is a flexible deep learning model for missing data imputation that accommodates all three types of missing data mechanisms: Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR). While it is particularly well-suited to MNAR scenarios where missingness patterns carry informative signals, CISS-VAE also functions effectively under MAR assumptions.\n",
    " \n",
    "A key feature of CISS-VAE is the use of unsupervised clustering to capture distinct patterns of missingness. Alongside cluster-specific representations, the method leverages shared encoder and decoder layers. This allows for knowledge transfer across clusters and enhances parameter stability, which is especially important when some clusters have small sample sizes. In situations where the data do not naturally partition into meaningful clusters, the model defaults to a pooled representation, preventing unnecessary complications from cluster-specific components.\n",
    " \n",
    "Additionally, CISS-VAE incorporates an iterative learning procedure, with a validation-based convergence criterion recommended to avoid overfitting. This procedure significantly improves imputation accuracy compared to traditional Variational Autoencoder training approaches in the presence of missing values. Overall, CISS-VAE adapts across a range of missing data mechanisms, leveraging clustering only when it offers clear benefits, and delivering robust, accurate imputations under varying conditions of missingness.\n",
    "\n",
    "There are two ways to run the CISS-VAE process. If you know what model\n",
    "parameters you want to use, you can use the {py:func}`ciss_vae.training.run_cissvae.run_cissvae` function to\n",
    "run the model once for the given set of parameters. If you want to tune\n",
    "the model instead, you can use {py:func}`ciss_vae.training.autotune.autotune`.\n",
    "\n",
    "The R package associated with this model can be found at [rCISS-VAE] (https://ciss-vae.github.io/rCISS-VAE/).\n",
    "\n",
    "# Installation\n",
    "\n",
    "The CISS-VAE package is currently available for python, with an R\n",
    "package to be released soon. It can be installed from either\n",
    "[github](https://github.com/CISS-VAE/CISS-VAE-python) or PyPI.\n",
    "\n",
    "``` bash\n",
    "# From PyPI (not released yet)\n",
    "pip install ciss-vae\n",
    "\n",
    "```\n",
    "\n",
    "``` bash\n",
    "# From GitHub (latest development version)\n",
    "pip install git+https://github.com/CISS-VAE/CISS-VAE-python.git\n",
    "```\n",
    "\n",
    "<div>\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> If you want run_cissvae to handle clustering, please install the\n",
    "> clustering dependencies scikit-learn, leidenalg, python-igraph with pip.\n",
    ">\n",
    "> ``` bash\n",
    "> pip install scikit-learn leidenalg python-igraph\n",
    ">\n",
    "> OR\n",
    ">\n",
    "> pip install ciss-vae[clustering]\n",
    "> ```\n",
    "\n",
    "</div>\n",
    "\n",
    "# Running the CISS-VAE Model\n",
    "\n",
    "You can use your own dataset or load the example dataset included with this package. \n",
    "\n",
    "To load the sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from ciss_vae.data import load_example_dataset\n",
    "df_missing, df_complete, clusters = load_example_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you already know what parameters you want for your model (or do not want to use the {py:func}`ciss_vae.training.autotune.autotune` function), you can use the {py:func}`ciss_vae.training.run_cissvae.run_cissvae` function to perform the imputation.\n",
    "\n",
    "The input dataset should be one of the following:\n",
    "\n",
    "    - A Pandas DataFrame  \n",
    "\n",
    "    - A NumPy array  \n",
    "\n",
    "    - A PyTorch tensor  \n",
    "\n",
    "Missing values should be represented using np.nan or None.\n",
    "\n",
    "**Assigning cluster labels**  \n",
    "There are three options for assigning cluster labels to the data:    \n",
    "    1. Manually assign cluster labels and provide them to the function via the `clusters` argument.     \n",
    "    2. Let `run_cissvae()` determine clusters based on patterns of missingness in the data by setting `clusters = None`.  \n",
    "        - To use Kmeans clustering, set n_clusters to the desired number of clusters.    \n",
    "        - To use Leiden Clustering clustering, leave `n_clusters = None`.       \n",
    "    3. To cluster on proportion of missingness, leave `clusters = None` and provide a missingness proportion matrix with the `missingness_proportion_matrix` argument.   \n",
    "        - To use Kmeans clustering, set n_clusters to the desired number of clusters.     \n",
    "        - To use Leiden Clustering clustering, leave `n_clusters = None`.      \n",
    "    <details>\n",
    "    <summary>Click for more details on missingness proportion matrix</summary>\n",
    "    <p>For the missingness proportion matrix, (either a `pandas.DataFrame` or `numpy.ndarray`) the rows should correspond to samples, the columns correspond to features, and the values are proportion of missingness of each feature for each sample. For features with multiple timepoints (like biomarker data collected at multiple visits), you may choose to have one column per feature and let the value be the overall proportion of missingness for that feature across all timepoints. See the <a href=\"https://ciss-vae.readthedocs.io/en/latest/missingness_prop_vignette.html\">clustering on missingness proportion tutorial</a> for more details.</p>\n",
    "    </details>   \n",
    "           \n",
    "\n",
    "To run the CISSVAE model with default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster dataset:\n",
      " ClusterDataset(n_samples=8000, n_features=30, n_clusters=4)\n",
      "  • Original missing: 61800 / 200000 (30.90%)\n",
      "  • Validation held-out: 13783 (9.97% of non-missing)\n",
      "  • .data shape:     (8000, 30)\n",
      "  • .masks shape:    (8000, 30)\n",
      "  • .val_data shape: (8000, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ciss_vae.training.run_cissvae import run_cissvae\n",
    "from ciss_vae.data import load_example_dataset\n",
    "\n",
    "# optional, display vae architecture\n",
    "from ciss_vae.utils.helpers import plot_vae_architecture\n",
    "\n",
    "data, _, clusters = load_example_dataset()\n",
    "\n",
    "imputed_data, vae = run_cissvae(data = data,\n",
    "## Dataset params\n",
    "    val_proportion = 0.1, ## Fraction of non-missing data held out for validation. Can input a list here if you want different proportions for different clusters. \n",
    "    replacement_value = 0.0, \n",
    "    columns_ignore = data.columns[:5], ## columns to ignore when selecting validation dataset (and clustering if you do not provide clusters). For example, demographic columns with no missingness.\n",
    "    print_dataset = True, \n",
    "\n",
    "## Cluster params\n",
    "    clusters = clusters, ## Where your cluster list goes. If none, will do clustering for you  \n",
    "    n_clusters = None, ## If you want run_cissvae to do clustering and you know how many clusters your data should have, enter that number here\n",
    "    # -- Params for Leiden Clustering --\n",
    "    k_neighbors = 15,\n",
    "    leiden_resolution = 0.5, ## Lower resolution = fewer clusters and bigger clusters, higher resolution = more, smaller clusters\n",
    "    leiden_objective = \"CPM\", \n",
    "    # -- End Params for Leiden Clustering --\n",
    "    seed = 42,\n",
    "    missingness_proportion_matrix = None,\n",
    "    \n",
    "## VAE model params\n",
    "    hidden_dims = [150, 120, 60], ## Dimensions of hidden layers, in order. One number per layer. \n",
    "    latent_dim = 15, ## Dimensions of latent embedding\n",
    "    layer_order_enc = [\"unshared\", \"unshared\", \"unshared\"], ## order of shared vs unshared layers for encode (can use u or s instead of unshared, shared)\n",
    "    layer_order_dec=[\"shared\", \"shared\",  \"shared\"],  ## order of shared vs unshared layers for decode\n",
    "    latent_shared=False, \n",
    "    output_shared=False, \n",
    "    batch_size = 4000, ## batch size for data loader\n",
    "    return_model = True, ## if true, outputs imputed dataset and model, otherwise just outputs imputed dataset. Set to true to return model for `plot_vae_architecture`\n",
    "\n",
    "## Initial Training params\n",
    "    epochs = 1000, ## default \n",
    "    initial_lr = 0.01, ## default\n",
    "    decay_factor = 0.999, ## default, factor learning rate is multiplied by after each epoch, prevents overfitting\n",
    "    beta= 0.001, ## default\n",
    "    device = None, ## If none, will use gpu if available, cpu if not. See torch.devices for info \n",
    "\n",
    "## Impute-refit loop params\n",
    "    max_loops = 100, ## max number of refit loops\n",
    "    patience = 2, ## number of loops to check after best_dataset updated. Can increase to avoid local extrema\n",
    "    epochs_per_loop = None, ## If none, same as epochs\n",
    "    initial_lr_refit = None, ## If none, picks up from end of initial training\n",
    "    decay_factor_refit = None, ## If none, same as decay_factor\n",
    "    beta_refit = None, ## if none, same as beta\n",
    "\n",
    "## Other params\n",
    "    verbose = False, \n",
    "    return_silhouettes = False, ## if true, will return silhouettes from clustering. If run_cissvae did not perform clustering, will return \"None\"\n",
    "    return_history = False, ## if true, will return training MSE history as pandas dataframe\n",
    "    return_clusters = False, ## if true, will return the cluster labels. Helpful if run_cissvae performs clustering\n",
    ")\n",
    "\n",
    "## OPTIONAL - PLOT VAE ARCHITECTURE\n",
    "plot_vae_architecture(model = vae,\n",
    "                        title = None, ## Set title of plot\n",
    "                        ## Colors below are default\n",
    "                        color_shared = \"skyblue\", \n",
    "                        color_unshared =\"lightcoral\",\n",
    "                        color_latent = \"gold\", # xx fix\n",
    "                        color_input = \"lightgreen\",\n",
    "                        color_output = \"lightgreen\",\n",
    "                        figsize=(16, 8),\n",
    "                        return_fig = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `return_clusters` parameter to get the cluster labels.\n",
    "Use the `return_history` parameter to get a dataframe with the training MSE history.\n",
    "\n",
    "### Avoiding imputation of certain entries\n",
    "In some cases, not all missing entries in a dataset are viable for imputation. For example, biomarker values after time of death would not necessarily be reasonable to impute and therefore should be ignored during the impute-refit training loop. To set certain data entries as un-imputable, create a do_not_impute matrix of the same size as the dataset, with 0 for entries that are non-missing or viable for imputation and 1 for entries that are missing and non-viable for imputation. \n",
    "\n",
    "This matrix (or pandas.Dataframe) can then be passed to the `run_cissvae()` function. Make sure that the do_not_impute matrix has the same column labels as the original data set to use the `cols_ignore` option to ignore certain columns during imputation. \n",
    "\n",
    "For more information, see the [dni vignette](https://ciss-vae.readthedocs.io/en/latest/dni_vignette.html) \n",
    "\n",
    "# Hyperparameter Tuning with Optuna\n",
    "\n",
    "The {py:func}`ciss_vae.training.autotune.autotune` function lets you tune the model's hyperparameters with\n",
    "optuna to get the best possible model.\n",
    "\n",
    "## Dataset Preparation\n",
    "\n",
    "Your dataset should be one of the following:\n",
    "\n",
    "    - A Pandas DataFrame  \n",
    "\n",
    "    - A NumPy array  \n",
    "\n",
    "    - A PyTorch tensor  \n",
    "\n",
    "Missing values should be represented using np.nan or None.\n",
    "\n",
    "Once the dataset is loaded, the first step is to identify patterns of\n",
    "missingness using clustering.\n",
    "\n",
    "## Clustering on missingness pattern\n",
    "\n",
    "Before fitting the model, the dataset should clustered based on its\n",
    "missingness pattern (i.e., which variables are missing in each\n",
    "observation).\n",
    "\n",
    "The built-in function can perfrom either leiden clustering or Kmeans clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mciss_vae\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_on_missing\n",
      "\u001b[32m      3\u001b[39m clusters, _ = cluster_on_missing(\n",
      "\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mdata\u001b[49m, \n",
      "\u001b[32m      5\u001b[39m     cols_ignore=\u001b[38;5;28;01mNone\u001b[39;00m, \n",
      "\u001b[32m      6\u001b[39m     n_clusters=\u001b[38;5;28;01mNone\u001b[39;00m, \n",
      "\u001b[32m      7\u001b[39m     k_neigbors=\u001b[32m500\u001b[39m, \u001b[38;5;66;03m## use higher k for fewer clusters generally\u001b[39;00m\n",
      "\u001b[32m      8\u001b[39m     use_snn=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[32m      9\u001b[39m     leiden_resolution=\u001b[32m0.005\u001b[39m, \u001b[38;5;66;03m## higher resolution -> more clusters\u001b[39;00m\n",
      "\u001b[32m     10\u001b[39m     leiden_objective=\u001b[33m\"\u001b[39m\u001b[33mCPM\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m     11\u001b[39m     seed=\u001b[32m42\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from ciss_vae.utils.clustering import cluster_on_missing\n",
    "\n",
    "clusters, _ = cluster_on_missing(\n",
    "    data, \n",
    "    cols_ignore=None, \n",
    "    n_clusters=None, \n",
    "    k_neigbors=500, ## use higher k for fewer clusters generally\n",
    "    use_snn=True,\n",
    "    leiden_resolution=0.005, ## higher resolution -> more clusters\n",
    "    leiden_objective=\"CPM\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This function uses Leiden Clustering clustering to detect structure in binary\n",
    "missingness masks, and will automatically determine the number of\n",
    "clusters if not specified. If n_clusters is specified, uses KMeans.\n",
    "\n",
    "**Options:**  \n",
    "- cols_ignore: list of columns to exclude when computing the missingness\n",
    "pattern. Ex: identifiers\n",
    "- n_clusters: set this to use K-Means instead of nonparametric\n",
    "    clustering.  \n",
    "\n",
    "**To cluster on proportion of missingness, see (tutorial)[https://ciss-vae.readthedocs.io/en/latest/missingness_prop_vignette.html] for more details.**\n",
    "\n",
    "# Creating a `ClusterDataset` object\n",
    "\n",
    "After obtaining cluster labels, construct a {py:class}`ciss_vae.classes.cluster_dataset.ClusterDataset`. This is the object that is fed into the autotune function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from ciss_vae.classes.cluster_dataset import ClusterDataset\n",
    "from ciss_vae.training.autotune import SearchSpace, autotune\n",
    "\n",
    "dataset = ClusterDataset(data = data,\n",
    "cluster_labels = clusters,\n",
    "val_proportion = 0.1, ## 10% non-missing data is default. You can also input a list if you want different val_proportions for each cluster\n",
    "replacement_value = 0, ## value to replace all missing data with before running model. Could be set to 0 or random\n",
    "columns_ignore = data.columns[:5] ## Tells ClusterDataset not to hold out entries demographic columns for validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create a SearchSpace object:\n",
    "\n",
    "In the SearchSpace object, define the search space for each\n",
    "hyperparameter. Each of the parameters in {py:class}`ciss_vae.training.autotune.SearchSpace` can be set as\n",
    "either tunable or non-tunable.\n",
    "\n",
    "Types of parameters:  \n",
    "- (min, max, step) -\\> creates a range   \n",
    "- \\[a, b, c\\] -\\> select value from list\n",
    "- x -\\> set param as non-tunable    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "## These are the default parameters. Please note these parameters may not be best for all datasets depending on size and complexity.\n",
    "\n",
    "searchspace = SearchSpace(\n",
    "                 num_hidden_layers=(1, 4), ## Set number of hidden layers\n",
    "                 hidden_dims=[64, 512], ## Allowable dimensions of hidden layers\n",
    "                 latent_dim=[10, 100],\n",
    "                 latent_shared=[True, False],\n",
    "                 output_shared=[True,False],\n",
    "                 lr=(1e-4, 1e-3),\n",
    "                 decay_factor=(0.9, 0.999),\n",
    "                 beta=0.01,\n",
    "                 num_epochs=1000,\n",
    "                 batch_size=64,\n",
    "                 num_shared_encode=[0, 1, 3],\n",
    "                 num_shared_decode=[0, 1, 3],\n",
    "                 encoder_shared_placement = [\"at_end\", \"at_start\", \"alternating\", \"random\"], ## where should the shared layers be placed in the encoder\n",
    "                 decoder_shared_placement = [\"at_end\", \"at_start\", \"alternating\", \"random\"], ## where should the shared layers be placed in the decoder\n",
    "                 refit_patience=2,\n",
    "                 refit_loops=100,\n",
    "                 epochs_per_loop = 1000,\n",
    "                 reset_lr_refit = [True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the `autotune` function:\n",
    "\n",
    "Once the search space is set, the autotune function can be run.\n",
    "\n",
    "There are a few options for running the autotune function, depending on your goals.   \n",
    "    1. Default:     \n",
    "        - Tune on a random sample of parameters from the SearchSpace obbject. This is the traditional autotune behavior.   \n",
    "        - For this behavior, set `constant_layer_size=False` and `evaluate_all_orders=False`.  \n",
    "    2. Tune with constant layer size:  \n",
    "        - All layers will be the same size. The size will be one selected from searchspace. \n",
    "        (ex: if `searchspace.num_hidden_layers` = [64, 512], all layers will be 64 or all layers will be 512),  \n",
    "        - For this behavior, set `constant_layer_size=False`  \n",
    "    3. Tune for all permutations of shared layer placement:  \n",
    "        - Will tune all possible layer orders/placements. Use `max_exhaustive_orders` to set a cap on the number of permutations to try.  \n",
    "        - For this behavior, set `evaluate_all_orders=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "best_imputed_df,  best_model, study, results_df = autotune(\n",
    "    search_space = searchspace,\n",
    "    train_dataset = dataset,                   # ClusterDataset object\n",
    "    save_model_path=None,\n",
    "    save_search_space_path=None,\n",
    "    n_trials=20,\n",
    "    study_name=\"vae_autotune\",                 # Default study name\n",
    "    device_preference=\"cuda\",\n",
    "    show_progress=False,                       # Show progress bar for training\n",
    "    optuna_dashboard_db=None,                  # If using optuna dashboard set db location here\n",
    "    load_if_exists=True,                       # If using optuna dashboard, if study by 'study_name' already exists, will load that study\n",
    "    seed = 42,                                 # Sets seed for random order of shared/unshared layers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## (optional) Using Optuna Dashboard\n",
    "\n",
    "You can use [optuna\n",
    "dashboard](https://optuna-dashboard.readthedocs.io/en/stable/getting-started.html)\n",
    "to visualize the importance of your tuning parameters. If you use VSCode\n",
    "or [Positron](https://positron.posit.co/download.html) there is an\n",
    "extension for viewing optuna dashboards in your development environment.\n",
    "\n",
    "![Screenshot of Optuna Dashboard](optuna_dash_v1.png)\n",
    "\n",
    "The optuna database file can be opened via commandline as well. (tutorial)[https://optuna-dashboard.readthedocs.io/en/stable/getting-started.html#command-line-interface]  \n",
    "\n",
    "\n",
    "To use optuna dashboard, set your database url in the autotune function. You\n",
    "can have multiple autotune 'studies' in the same database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "best_imputed_df,  best_model, study, results_df = autotune(\n",
    "    search_space = searchspace,\n",
    "    train_dataset = dataset,                   # 'ClusterDataset' object\n",
    "    save_model_path=None,\n",
    "    save_search_space_path=None,\n",
    "    n_trials=20,\n",
    "    study_name=\"vae_autotune\",                 # Default study name\n",
    "    device_preference=\"cuda\",\n",
    "    show_progress=False,                       # Show progress bar for training\n",
    "    optuna_dashboard_db=\"sqlite:///db.sqlite3\",                  # If using optuna dashboard set db location here, otherwise set to None\n",
    "    load_if_exists=True,                       # Continues previous study by study_name if one exists. If false, will give error if study_name already exists in the set dashboard\n",
    "    seed = 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Saving and loading models\n",
    "\n",
    "## Saving\n",
    "\n",
    "If you want to save your model and load it later, there are two options.\n",
    "\n",
    "To save the model weights after training:\n",
    "\n",
    "``` python\n",
    "## assuming your trained model is called 'model'\n",
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), \"trained_vae.pt\")\n",
    "```\n",
    "\n",
    "If you want to save the entire model:\n",
    "\n",
    "``` python\n",
    "torch.save(model, \"trained_vae_full.pt\")\n",
    "```\n",
    "\n",
    "## Loading a Model\n",
    "\n",
    "To reload the model for imputation or further training:   \n",
    "1. Re-create the model architecture with the same settings used during training  \n",
    "2. Load the saved weights  \n",
    "\n",
    "``` python\n",
    "from ciss_vae.classes.vae import CISSVAE\n",
    "\n",
    "# 1. Define the architecture (must match the saved model!)\n",
    "model = CISSVAE(\n",
    "    input_dim=...,\n",
    "    hidden_dims=[...],\n",
    "    layer_order_enc=[...],\n",
    "    layer_order_dec=[...],\n",
    "    latent_shared=...,\n",
    "    num_clusters=...,\n",
    "    latent_dim=...,\n",
    "    output_shared=...\n",
    ")\n",
    "model.load_state_dict(torch.load(\"trained_vae.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "## optional to get imputed dataset. \n",
    "from ciss_vae.utils.helpers import get_imputed_df\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "## assuming dataset is a ClusterDataset\n",
    "data_loader =  DataLoader(dataset, batch_size=4000)\n",
    "\n",
    "imputed_df = get_imputed_df(model, data_loader)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}