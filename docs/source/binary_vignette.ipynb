{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling binary data columns\n",
    "\n",
    "The CISS-VAE model can handle binary and categorical variables in addition to continuous ones. Categorical variables must be represented with binary dummy variables. \n",
    "\n",
    "When imputing binary data, we apply a sigmoid activation function at the end to convert to a probability. Because some datasets have both binary and continuous variables, you can input a binary variable mask (boolean vector) to tell the model which variables are binary so it acts accordingly. \n",
    "\n",
    "## Example dataset\n",
    "\n",
    "The example dataset below has both binary and continuous variables in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X matrix:\n",
      "    feat1  feat2  feat3  feat4  feat5  bf1  bf2  bf3  bf4  bf5\n",
      "0      52    2.0    6.0    4.0    2.0  NaN  NaN  NaN  NaN  0.0\n",
      "1      93    1.0    7.0    NaN    2.0  NaN  1.0  1.0  1.0  1.0\n",
      "2      15    4.0    NaN    4.0    2.0  0.0  0.0  1.0  0.0  NaN\n",
      "3      72    4.0    3.0    NaN    1.0  NaN  1.0  NaN  1.0  NaN\n",
      "4      61    NaN    1.0    NaN    1.0  0.0  NaN  NaN  NaN  0.0\n",
      "..    ...    ...    ...    ...    ...  ...  ...  ...  ...  ...\n",
      "95     85    NaN    NaN    6.0    2.0  0.0  0.0  1.0  NaN  1.0\n",
      "96     80    1.0    NaN    7.0    2.0  0.0  NaN  1.0  NaN  1.0\n",
      "97     82    2.0    4.0    NaN    1.0  0.0  NaN  1.0  NaN  NaN\n",
      "98     53    4.0    3.0    7.0    7.0  NaN  0.0  0.0  0.0  1.0\n",
      "99     24    4.0    1.0    3.0    NaN  0.0  1.0  1.0  1.0  0.0\n",
      "\n",
      "[100 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_rows = 100\n",
    "prop_mask = 0.3\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    \"feat1\": np.random.choice(np.arange(1, n_rows + 1), size=n_rows, replace=True),\n",
    "    \"feat2\": np.random.choice(np.arange(1, 6), size=n_rows, replace=True),\n",
    "    \"feat3\": np.random.choice(np.arange(1, 8), size=n_rows, replace=True),\n",
    "    \"feat4\": np.random.choice(np.arange(1, 8), size=n_rows, replace=True),\n",
    "    \"feat5\": np.random.choice(np.arange(1, 8), size=n_rows, replace=True),\n",
    "    ## now add some binary features\n",
    "    \"bf1\": np.random.binomial(1, 0.25, size=n_rows),\n",
    "    \"bf2\": np.random.binomial(1, 0.5, size=n_rows),\n",
    "    \"bf3\": np.random.binomial(1, 0.75, size=n_rows),\n",
    "    \"bf4\": np.random.binomial(1, 0.33, size=n_rows),\n",
    "    \"bf5\": np.random.binomial(1, 0.66, size=n_rows),\n",
    "})\n",
    "\n",
    "X_raw = X.copy()\n",
    "\n",
    "for col in X.columns[1:]:  # skip feat1\n",
    "    idx = np.where(~X[col].isna())[0]  # indices of non-NA entries\n",
    "    n_mask = int(np.ceil(len(idx) * prop_mask))\n",
    "    if n_mask > 0:\n",
    "        mask_idx = np.random.choice(idx, size=n_mask, replace=False)\n",
    "        X.loc[mask_idx, col] = np.nan\n",
    "\n",
    "print(f\"X matrix:\\n{X}\")\n",
    "\n",
    "\n",
    "## choosing random clusters for now \n",
    "clusters = np.random.choice([1, 2, 3], size=n_rows, replace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Binary Vector\n",
    "\n",
    "The binary vector `binary_feature_mask` is of length p for an n x p data matrix and is `True` for binary columns and `False` for continuous columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "binary_vector = [False, False, False, False, False, True, True, True, True, True]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `run_cissvae()` with binary matrix\n",
    "\n",
    "Pass the binary vector to the `run_cissvae()` function using the `binary_feature_mask` argument. Note: even if columns are ignored via `columns_ignore`, those columns must be included in the `binary_feature_mask`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nfs/vaithid1/CISS-VAE/CISS-VAE/src/ciss_vae/__init__.py\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n",
      "\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mciss_vae\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhelpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_vae_architecture\n",
      "\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(ciss_vae.\u001b[34m__file__\u001b[39m)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m imputed_data, vae, ds, history = \u001b[43mrun_cissvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m      8\u001b[39m \u001b[38;5;66;43;03m## Dataset params\u001b[39;49;00m\n",
      "\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns_ignore\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## columns to ignore when selecting validation dataset (and clustering if you do not provide clusters). For example, demographic columns with no missingness.\u001b[39;49;00m\n",
      "\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclusters\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mclusters\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbinary_feature_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_vector\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     13\u001b[39m \u001b[38;5;66;43;03m## VAE model params\u001b[39;49;00m\n",
      "\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## Dimensions of hidden layers, in order. One number per layer. \u001b[39;49;00m\n",
      "\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## Dimensions of latent embedding\u001b[39;49;00m\n",
      "\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_order_enc\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munshared\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munshared\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munshared\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## order of shared vs unshared layers for encode (can use u or s instead of unshared, shared)\u001b[39;49;00m\n",
      "\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_order_dec\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshared\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshared\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshared\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m## order of shared vs unshared layers for decode\u001b[39;49;00m\n",
      "\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_shared\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_shared\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## batch size for data loader\u001b[39;49;00m\n",
      "\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## if true, outputs imputed dataset and model, otherwise just outputs imputed dataset. Set to true to return model for `plot_vae_architecture`\u001b[39;49;00m\n",
      "\u001b[32m     22\u001b[39m \n",
      "\u001b[32m     23\u001b[39m \u001b[38;5;66;43;03m## Initial Training params\u001b[39;49;00m\n",
      "\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## default \u001b[39;49;00m\n",
      "\u001b[32m     25\u001b[39m \n",
      "\u001b[32m     26\u001b[39m \u001b[38;5;66;43;03m## Other params\u001b[39;49;00m\n",
      "\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_history\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## if true, will return training MSE history as pandas dataframe\u001b[39;49;00m\n",
      "\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n",
      "\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe successfully imputed dataset:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mimputed_data.head\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CISS-VAE/CISS-VAE/src/ciss_vae/training/run_cissvae.py:350\u001b[39m, in \u001b[36mrun_cissvae\u001b[39m\u001b[34m(data, val_proportion, replacement_value, columns_ignore, print_dataset, imputable_matrix, binary_feature_mask, clusters, n_clusters, k_neighbors, leiden_resolution, leiden_objective, seed, missingness_proportion_matrix, scale_features, hidden_dims, latent_dim, layer_order_enc, layer_order_dec, latent_shared, output_shared, batch_size, return_model, epochs, initial_lr, decay_factor, weight_decay, beta, device, max_loops, patience, epochs_per_loop, initial_lr_refit, decay_factor_refit, beta_refit, verbose, return_clusters, return_silhouettes, return_history, return_dataset, debug)\u001b[39m\n",
      "\u001b[32m    336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m    337\u001b[39m     vae = train_vae_initial(\n",
      "\u001b[32m    338\u001b[39m     model=vae,\n",
      "\u001b[32m    339\u001b[39m     train_loader=train_loader,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    347\u001b[39m     return_history = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[32m    348\u001b[39m     )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m imputed_dataset, vae, _ = \u001b[43mimpute_and_refit_loop\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_loops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_loops\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs_per_loop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs_per_loop\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_lr_refit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## should start from last learning rate\u001b[39;49;00m\n",
      "\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecay_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecay_factor_refit\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta_refit\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    363\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# ----------------\u001b[39;00m\n",
      "\u001b[32m    366\u001b[39m \u001b[38;5;66;03m# Construct history dataframe\u001b[39;00m\n",
      "\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# ----------------\u001b[39;00m\n",
      "\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_history:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CISS-VAE/CISS-VAE/src/ciss_vae/training/train_refit.py:352\u001b[39m, in \u001b[36mimpute_and_refit_loop\u001b[39m\u001b[34m(model, train_loader, max_loops, patience, epochs_per_loop, initial_lr, decay_factor, weight_decay, beta, device, verbose, batch_size, progress_epoch)\u001b[39m\n",
      "\u001b[32m    349\u001b[39m data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[32m    350\u001b[39m \u001b[38;5;66;03m#final_imputed = get_imputed(best_model, data_loader, device)\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m imputed_df = \u001b[43mget_imputed_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose: \n",
      "\u001b[32m    355\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest Val MSE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_mse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Imputed Dataset MSE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_val_mse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CISS-VAE/CISS-VAE/src/ciss_vae/utils/helpers.py:257\u001b[39m, in \u001b[36mget_imputed_df\u001b[39m\u001b[34m(model, data_loader, device)\u001b[39m\n",
      "\u001b[32m    255\u001b[39m     out[:, ccols] = full_x[:, ccols] * stds[ccols] + means[ccols]\n",
      "\u001b[32m    256\u001b[39m     \u001b[38;5;66;03m# Missing values → denorm the model reconstructions\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     out[missing_mask[:, ccols]] = \u001b[43m(\u001b[49m\u001b[43mrecon\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mstds\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmissing_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mccols\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# 3) Binary columns:\u001b[39;00m\n",
      "\u001b[32m    260\u001b[39m \u001b[38;5;66;03m#    - Observed entries: denormalize back to original scale (≈0/1) then clamp to [0,1]\u001b[39;00m\n",
      "\u001b[32m    261\u001b[39m \u001b[38;5;66;03m#    - Missing entries: use model probabilities directly (already in [0,1])\u001b[39;00m\n",
      "\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bin_1d.any():\n",
      "\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "import ciss_vae\n",
    "\n",
    "from ciss_vae.training.run_cissvae import run_cissvae\n",
    "from ciss_vae.utils.helpers import plot_vae_architecture\n",
    "print(ciss_vae.__file__)\n",
    "\n",
    "imputed_data, vae, ds, history = run_cissvae(data = X,\n",
    "## Dataset params\n",
    "    columns_ignore = X.columns[0], ## columns to ignore when selecting validation dataset (and clustering if you do not provide clusters). For example, demographic columns with no missingness.\n",
    "    clusters = clusters,\n",
    "    print_dataset = False,\n",
    "    binary_feature_mask = binary_vector,\n",
    "## VAE model params\n",
    "    hidden_dims = [150, 120, 60], ## Dimensions of hidden layers, in order. One number per layer. \n",
    "    latent_dim = 15, ## Dimensions of latent embedding\n",
    "    layer_order_enc = [\"unshared\", \"unshared\", \"unshared\"], ## order of shared vs unshared layers for encode (can use u or s instead of unshared, shared)\n",
    "    layer_order_dec=[\"shared\", \"shared\",  \"shared\"],  ## order of shared vs unshared layers for decode\n",
    "    latent_shared=False, \n",
    "    output_shared=False, \n",
    "    batch_size = 4000, ## batch size for data loader\n",
    "    return_model = True, ## if true, outputs imputed dataset and model, otherwise just outputs imputed dataset. Set to true to return model for `plot_vae_architecture`\n",
    "\n",
    "## Initial Training params\n",
    "    epochs = 5, ## default \n",
    "\n",
    "## Other params\n",
    "    return_history = True, ## if true, will return training MSE history as pandas dataframe\n",
    "    return_dataset=True,\n",
    "    debug = False\n",
    ")\n",
    "\n",
    "print(f\"The successfully imputed dataset:\\n{imputed_data.head}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"History \\n{history}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, the vae architecture can be printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "plot_vae_architecture(model = vae,\n",
    "                        title = None, ## Set title of plot\n",
    "                        ## Colors below are default\n",
    "                        color_shared = \"skyblue\", \n",
    "                        color_unshared =\"lightcoral\",\n",
    "                        color_latent = \"gold\", # xx fix\n",
    "                        color_input = \"lightgreen\",\n",
    "                        color_output = \"lightgreen\",\n",
    "                        figsize=(16, 8),\n",
    "                        return_fig = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Binary Feature Mask with Autotune\n",
    "\n",
    "To use a `binary_feature_mask` with `autotune()`, pass the use the `binary_feature_mask` parameter when initializing the `ClusterDataset` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from ciss_vae.classes.cluster_dataset import ClusterDataset\n",
    "from ciss_vae.training.autotune import autotune, SearchSpace\n",
    "cd = ClusterDataset(\n",
    "    X, cluster_labels = clusters, binary_feature_mask=binary_vector\n",
    ")\n",
    "\n",
    "ss = SearchSpace(\n",
    "    num_hidden_layers = [1, 2],\n",
    "    hidden_dims = [6, 16, 32],\n",
    "    latent_dim=10,\n",
    "    latent_shared=True,\n",
    "    output_shared = True,\n",
    "    lr = 0.01,\n",
    "    decay_factor=0.999,\n",
    "    num_epochs = 100,\n",
    "    num_shared_encode = 1,\n",
    "    num_shared_decode = 1,\n",
    "    epochs_per_loop=100,\n",
    "    reset_lr_refit=False\n",
    "\n",
    ")\n",
    "autotune(search_space = ss, train_dataset = cd, optuna_dashboard_db =  \"sqlite:///optuna_study_test_binary.db\", debug = True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
