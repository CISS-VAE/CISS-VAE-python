{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding imputation undesired data entries\n",
    "\n",
    "To avoid imputing certain missing datapoints, construct a `imputable_matrix`(DNI matrix) of the same size and shape as the original dataset. For this matrix, use 1 to represent values in the original dataset that are missing and viable for imputation or non-missing. Use 0 to represent values in the original dataset that are missing but not viable for imputation (for example, measurements at timepoints occuring after death). To use the `cols_ignore` argument in `run_cissvae` or `ClusterDataset()`, make sure that the `imputable_matrix` has the same column names/indices as the original dataset. \n",
    "\n",
    "## Example using included dataset\n",
    "\n",
    "First load the dataset and load or create the DNI matrix. Here we can see that [1, Y12], [2, Y22], and [1, Y52] are marked as non-imputable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nfs/vaithid1/CISS-VAE/CISS-VAE/src/ciss_vae/__init__.py\n",
      "Df missing:\n",
      "        Y11  Y12        Y13        Y14        Y15        Y21  Y22        Y23  \\\n",
      "0 -4.049537  NaN        NaN -14.369151 -17.564448        NaN  NaN -35.772630   \n",
      "1  0.546168  NaN -12.189518  -7.722474        NaN  -7.470250  NaN -25.924360   \n",
      "2       NaN  NaN -20.358905 -15.126494 -17.251376 -18.448422  NaN -34.400862   \n",
      "\n",
      "         Y24        Y25  ...       Y41  Y42       Y43       Y44       Y45  \\\n",
      "0 -28.098906 -30.242588  ... -0.904960  NaN       NaN -3.694852 -5.680293   \n",
      "1 -17.231424 -18.695290  ...  2.624586  NaN -5.776195 -1.379495 -2.329604   \n",
      "2 -27.250598 -28.839809  ...       NaN  NaN -7.215718 -3.350797 -6.895340   \n",
      "\n",
      "        Y51  Y52       Y53       Y54       Y55  \n",
      "0  2.587588  NaN -4.681195 -2.248406 -2.679081  \n",
      "1  6.080512  NaN -2.290062 -0.887398  0.562532  \n",
      "2  2.531148  NaN -5.427430 -1.330163 -2.324382  \n",
      "\n",
      "[3 rows x 25 columns], \n",
      "\n",
      "Do not impute:\n",
      "   Y11  Y12  Y13  Y14  Y15  Y21  Y22  Y23  Y24  Y25  ...  Y41  Y42  Y43  Y44  \\\n",
      "0    1    1    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
      "1    1    0    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
      "2    1    1    1    1    1    1    0    1    1    1  ...    1    1    1    1   \n",
      "\n",
      "   Y45  Y51  Y52  Y53  Y54  Y55  \n",
      "0    1    1    1    1    1    1  \n",
      "1    1    1    0    1    1    1  \n",
      "2    1    1    1    1    1    1  \n",
      "\n",
      "[3 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ciss_vae.data import load_example_dataset, load_dni\n",
    "import ciss_vae\n",
    "print(ciss_vae.__file__)\n",
    "\n",
    "df_missing, _, clusters = load_example_dataset()\n",
    "\n",
    "dni = load_dni()\n",
    "\n",
    "dni.columns = df_missing.columns\n",
    "\n",
    "print(f\"Df missing:\\n{df_missing.head(3).drop(df_missing.columns[:5].to_list(), axis=1)}, \\n\\nDo not impute:\\n{dni.head(3).drop(df_missing.columns[:5].to_list(), axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `run_cissvae()` with DNI matrix\n",
    "\n",
    "The `run_cissvae()` function can accept the DNI matrix as an input. Make sure that the column names of the DNI matrix match those of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mciss_vae\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrun_cissvae\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_cissvae\n",
      "\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mciss_vae\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhelpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_vae_architecture\n",
      "\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m imputed_data, vae, ds, history = \u001b[43mrun_cissvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_missing\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m      5\u001b[39m \u001b[38;5;66;43;03m## Dataset params\u001b[39;49;00m\n",
      "\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns_ignore\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_missing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## columns to ignore when selecting validation dataset (and clustering if you do not provide clusters). For example, demographic columns with no missingness.\u001b[39;49;00m\n",
      "\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimputable_matrix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdni\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclusters\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mclusters\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     10\u001b[39m \n",
      "\u001b[32m     11\u001b[39m \u001b[38;5;66;43;03m## VAE model params\u001b[39;49;00m\n",
      "\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## Dimensions of hidden layers, in order. One number per layer. \u001b[39;49;00m\n",
      "\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## Dimensions of latent embedding\u001b[39;49;00m\n",
      "\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_order_enc\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munshared\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munshared\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munshared\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## order of shared vs unshared layers for encode (can use u or s instead of unshared, shared)\u001b[39;49;00m\n",
      "\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_order_dec\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshared\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshared\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshared\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m## order of shared vs unshared layers for decode\u001b[39;49;00m\n",
      "\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_shared\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_shared\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## batch size for data loader\u001b[39;49;00m\n",
      "\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## if true, outputs imputed dataset and model, otherwise just outputs imputed dataset. Set to true to return model for `plot_vae_architecture`\u001b[39;49;00m\n",
      "\u001b[32m     20\u001b[39m \n",
      "\u001b[32m     21\u001b[39m \u001b[38;5;66;43;03m## Initial Training params\u001b[39;49;00m\n",
      "\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## default \u001b[39;49;00m\n",
      "\u001b[32m     23\u001b[39m \n",
      "\u001b[32m     24\u001b[39m \u001b[38;5;66;43;03m## Other params\u001b[39;49;00m\n",
      "\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_history\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## if true, will return training MSE history as pandas dataframe\u001b[39;49;00m\n",
      "\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n",
      "\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe successfully imputed dataset:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mimputed_data.head\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CISS-VAE/CISS-VAE/src/ciss_vae/training/run_cissvae.py:350\u001b[39m, in \u001b[36mrun_cissvae\u001b[39m\u001b[34m(data, val_proportion, replacement_value, columns_ignore, print_dataset, imputable_matrix, binary_feature_mask, clusters, n_clusters, k_neighbors, leiden_resolution, leiden_objective, seed, missingness_proportion_matrix, scale_features, hidden_dims, latent_dim, layer_order_enc, layer_order_dec, latent_shared, output_shared, batch_size, return_model, epochs, initial_lr, decay_factor, weight_decay, beta, device, max_loops, patience, epochs_per_loop, initial_lr_refit, decay_factor_refit, beta_refit, verbose, return_clusters, return_silhouettes, return_history, return_dataset, debug)\u001b[39m\n",
      "\u001b[32m    336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m    337\u001b[39m     vae = train_vae_initial(\n",
      "\u001b[32m    338\u001b[39m     model=vae,\n",
      "\u001b[32m    339\u001b[39m     train_loader=train_loader,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    347\u001b[39m     return_history = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[32m    348\u001b[39m     )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m imputed_dataset, vae, _ = \u001b[43mimpute_and_refit_loop\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_loops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_loops\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs_per_loop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs_per_loop\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_lr_refit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## should start from last learning rate\u001b[39;49;00m\n",
      "\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecay_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecay_factor_refit\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta_refit\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    363\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# ----------------\u001b[39;00m\n",
      "\u001b[32m    366\u001b[39m \u001b[38;5;66;03m# Construct history dataframe\u001b[39;00m\n",
      "\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# ----------------\u001b[39;00m\n",
      "\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_history:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CISS-VAE/CISS-VAE/src/ciss_vae/training/train_refit.py:352\u001b[39m, in \u001b[36mimpute_and_refit_loop\u001b[39m\u001b[34m(model, train_loader, max_loops, patience, epochs_per_loop, initial_lr, decay_factor, weight_decay, beta, device, verbose, batch_size, progress_epoch)\u001b[39m\n",
      "\u001b[32m    349\u001b[39m data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[32m    350\u001b[39m \u001b[38;5;66;03m#final_imputed = get_imputed(best_model, data_loader, device)\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m imputed_df = \u001b[43mget_imputed_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose: \n",
      "\u001b[32m    355\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest Val MSE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_mse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Imputed Dataset MSE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_val_mse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CISS-VAE/CISS-VAE/src/ciss_vae/utils/helpers.py:257\u001b[39m, in \u001b[36mget_imputed_df\u001b[39m\u001b[34m(model, data_loader, device)\u001b[39m\n",
      "\u001b[32m    255\u001b[39m     out[:, ccols] = full_x[:, ccols] * stds[ccols] + means[ccols]\n",
      "\u001b[32m    256\u001b[39m     \u001b[38;5;66;03m# Missing values → denorm the model reconstructions\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     out[missing_mask[:, ccols]] = \u001b[43m(\u001b[49m\u001b[43mrecon\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mstds\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmissing_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mccols\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# 3) Binary columns:\u001b[39;00m\n",
      "\u001b[32m    260\u001b[39m \u001b[38;5;66;03m#    - Observed entries: denormalize back to original scale (≈0/1) then clamp to [0,1]\u001b[39;00m\n",
      "\u001b[32m    261\u001b[39m \u001b[38;5;66;03m#    - Missing entries: use model probabilities directly (already in [0,1])\u001b[39;00m\n",
      "\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bin_1d.any():\n",
      "\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "from ciss_vae.training.run_cissvae import run_cissvae\n",
    "from ciss_vae.utils.helpers import plot_vae_architecture\n",
    "\n",
    "imputed_data, vae, ds, history = run_cissvae(data = df_missing,\n",
    "## Dataset params\n",
    "    columns_ignore = df_missing.columns[:5], ## columns to ignore when selecting validation dataset (and clustering if you do not provide clusters). For example, demographic columns with no missingness.\n",
    "    imputable_matrix=dni,\n",
    "    clusters = clusters,\n",
    "    print_dataset = False,\n",
    "    \n",
    "## VAE model params\n",
    "    hidden_dims = [150, 120, 60], ## Dimensions of hidden layers, in order. One number per layer. \n",
    "    latent_dim = 15, ## Dimensions of latent embedding\n",
    "    layer_order_enc = [\"unshared\", \"unshared\", \"unshared\"], ## order of shared vs unshared layers for encode (can use u or s instead of unshared, shared)\n",
    "    layer_order_dec=[\"shared\", \"shared\",  \"shared\"],  ## order of shared vs unshared layers for decode\n",
    "    latent_shared=False, \n",
    "    output_shared=False, \n",
    "    batch_size = 4000, ## batch size for data loader\n",
    "    return_model = True, ## if true, outputs imputed dataset and model, otherwise just outputs imputed dataset. Set to true to return model for `plot_vae_architecture`\n",
    "\n",
    "## Initial Training params\n",
    "    epochs = 5, ## default \n",
    "\n",
    "## Other params\n",
    "    return_history = True, ## if true, will return training MSE history as pandas dataframe\n",
    "    return_dataset=True\n",
    ")\n",
    "\n",
    "print(f\"The successfully imputed dataset:\\n{imputed_data.head}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can see that [1, Y12], [2, Y22], and [1, Y52] are still NaN, even though other missing entries have been imputed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Imputed dataset:\\n{imputed_data.drop(df_missing.columns[:5].to_list(), axis=1).head(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, the vae architecture can be printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "plot_vae_architecture(model = vae,\n",
    "                        title = None, ## Set title of plot\n",
    "                        ## Colors below are default\n",
    "                        color_shared = \"skyblue\", \n",
    "                        color_unshared =\"lightcoral\",\n",
    "                        color_latent = \"gold\", # xx fix\n",
    "                        color_input = \"lightgreen\",\n",
    "                        color_output = \"lightgreen\",\n",
    "                        figsize=(16, 8),\n",
    "                        return_fig = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also use the imputable maatrix with the autotune function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ciss_vae.data import load_example_dataset, load_dni\n",
    "import ciss_vae\n",
    "from ciss_vae.classes.cluster_dataset import ClusterDataset\n",
    "from ciss_vae.training.autotune import SearchSpace, autotune\n",
    "\n",
    "print(ciss_vae.__file__)\n",
    "\n",
    "df_missing, _, clusters = load_example_dataset()\n",
    "\n",
    "dni = load_dni()\n",
    "\n",
    "dataset = ClusterDataset(\n",
    "    data = df_missing,\n",
    "    cluster_labels = clusters,\n",
    "    imputable = dni,\n",
    "\n",
    ")\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "searchspace = SearchSpace(\n",
    "                 num_hidden_layers=(1, 4), ## Set number of hidden layers\n",
    "                 hidden_dims=[64, 512], ## Allowable dimensions of hidden layers\n",
    "                 latent_dim=[10, 100],\n",
    "                 latent_shared=[True, False],\n",
    "                 output_shared=[True,False],\n",
    "                 lr=(1e-4, 1e-3),\n",
    "                 decay_factor=(0.9, 0.999),\n",
    "                 beta=0.01,\n",
    "                 num_epochs=100,\n",
    "                 batch_size=64,\n",
    "                 num_shared_encode=[0, 1, 3],\n",
    "                 num_shared_decode=[0, 1, 3],\n",
    "                 encoder_shared_placement = [\"at_end\", \"at_start\", \"alternating\", \"random\"], ## where should the shared layers be placed in the encoder\n",
    "                 decoder_shared_placement = [\"at_end\", \"at_start\", \"alternating\", \"random\"], ## where should the shared layers be placed in the decoder\n",
    "                 refit_patience=2,\n",
    "                 refit_loops=10,\n",
    "                 epochs_per_loop = 100,\n",
    "                 reset_lr_refit = [True, False])\n",
    "\n",
    "results = autotune(\n",
    "    search_space = searchspace,\n",
    "    train_dataset = dataset,                   # ClusterDataset object\n",
    "    save_model_path=None,\n",
    "    save_search_space_path=None,\n",
    "    n_trials=20,\n",
    "    study_name=\"vae_autotune_v3\",                 # Default study name\n",
    "    device_preference=\"cuda\",\n",
    "    show_progress=True,                       # Show progress bar for training\n",
    "    optuna_dashboard_db=\"sqlite:///test_dni_python.sqlite3\",                  # If using optuna dashboard set db location here\n",
    "    load_if_exists=True,                       # If using optuna dashboard, if study by 'study_name' already exists, will load that study\n",
    "    seed = 42,     \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
